{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "0etRtS83RcWS"
   },
   "source": [
    "# SingleStore and Llama 3 RAG Quickstart"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "r1IzNLho-NqV"
   },
   "source": [
    "This notebook provides an example of how to use SingleStore as a vector database in conjunction with Llama 3. We'll be using Replicate to host an instance of Llama 3 ov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T16:23:26.745533Z",
     "iopub.status.busy": "2024-04-30T16:23:26.745247Z",
     "iopub.status.idle": "2024-04-30T16:23:29.724848Z",
     "shell.execute_reply": "2024-04-30T16:23:29.724195Z",
     "shell.execute_reply.started": "2024-04-30T16:23:26.745517Z"
    },
    "language": "python"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U langchain singlestoredb langchain-openai --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install replicate --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_community --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T16:20:57.102796Z",
     "iopub.status.busy": "2024-04-30T16:20:57.102518Z",
     "iopub.status.idle": "2024-04-30T16:21:15.655746Z",
     "shell.execute_reply": "2024-04-30T16:21:15.655228Z",
     "shell.execute_reply.started": "2024-04-30T16:20:57.102779Z"
    },
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "import os\n",
    "\n",
    "REPLICATE_API_TOKEN = getpass()\n",
    "\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T16:25:44.319879Z",
     "iopub.status.busy": "2024-04-30T16:25:44.319607Z",
     "iopub.status.idle": "2024-04-30T16:25:45.720634Z",
     "shell.execute_reply": "2024-04-30T16:25:45.720135Z",
     "shell.execute_reply.started": "2024-04-30T16:25:44.319863Z"
    },
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "OPENAI_API_KEY = getpass()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T16:21:22.269448Z",
     "iopub.status.busy": "2024-04-30T16:21:22.269189Z",
     "iopub.status.idle": "2024-04-30T16:21:22.272649Z",
     "shell.execute_reply": "2024-04-30T16:21:22.272009Z",
     "shell.execute_reply.started": "2024-04-30T16:21:22.269430Z"
    },
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import Replicate\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "m01XDoo4UQvN"
   },
   "source": [
    "## Initialize Llama 3 in Replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T16:23:00.001757Z",
     "iopub.status.busy": "2024-04-30T16:23:00.001504Z",
     "iopub.status.idle": "2024-04-30T16:23:08.353593Z",
     "shell.execute_reply": "2024-04-30T16:23:08.353034Z",
     "shell.execute_reply.started": "2024-04-30T16:23:00.001741Z"
    },
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SingleStoreDB is a distributed relational database that is designed to handle large amounts of data and scale horizontally. It is optimized for real-time analytics and machine learning workloads.\\n\\nSingleStoreDB is built from the ground up to be a cloud-native database. It is designed to take advantage of cloud computing, including scalability, high availability, and cost-effectiveness.\\n\\nSome of the key features of SingleStoreDB include:\\n\\n* Scalability: SingleStoreDB can scale horizontally to handle large amounts of data and increasing workloads.\\n* High availability: SingleStoreDB is designed to provide high availability, ensuring that your database is always available and accessible.\\n*'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = Replicate(\n",
    "    model=\"meta/meta-llama-3-8b-instruct\",\n",
    "    model_kwargs={\"temperature\": 0.75, \"max_length\": 500, \"top_p\": 1},\n",
    ")\n",
    "prompt = \"\"\"\n",
    "User: What is SingleStoreDB?\n",
    "Assistant:\n",
    "\"\"\"\n",
    "llm(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-26T08:44:11.362703Z",
     "iopub.status.busy": "2024-04-26T08:44:11.362215Z",
     "iopub.status.idle": "2024-04-26T08:44:11.364950Z",
     "shell.execute_reply": "2024-04-26T08:44:11.364515Z",
     "shell.execute_reply.started": "2024-04-26T08:44:11.362679Z"
    },
    "language": "python"
   },
   "source": [
    "## RAG over audio files using SingleStoreDB\n",
    "\n",
    "Now we will embed the text descriptions of the audio file(s). This allows us to search and retrieve relevant files for RAG later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T16:31:34.107933Z",
     "iopub.status.busy": "2024-04-30T16:31:34.107351Z",
     "iopub.status.idle": "2024-04-30T16:31:34.432853Z",
     "shell.execute_reply": "2024-04-30T16:31:34.432358Z",
     "shell.execute_reply.started": "2024-04-30T16:31:34.107910Z"
    },
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import SingleStoreDB\n",
    "import os\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "os.environ[\"SINGLESTOREDB_URL\"] = f'{connection_user}:{connection_password}@{connection_host}:{connection_port}/{connection_default_database}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T16:47:35.474206Z",
     "iopub.status.busy": "2024-04-30T16:47:35.473942Z",
     "iopub.status.idle": "2024-04-30T16:47:42.860963Z",
     "shell.execute_reply": "2024-04-30T16:47:42.860466Z",
     "shell.execute_reply.started": "2024-04-30T16:47:35.474190Z"
    },
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://aws.amazon.com/blogs/aws/metas-llama-3-models-are-now-available-in-amazon-bedrock/\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T16:47:43.868724Z",
     "iopub.status.busy": "2024-04-30T16:47:43.868477Z",
     "iopub.status.idle": "2024-04-30T16:47:43.872090Z",
     "shell.execute_reply": "2024-04-30T16:47:43.871572Z",
     "shell.execute_reply.started": "2024-04-30T16:47:43.868710Z"
    },
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T16:47:45.382362Z",
     "iopub.status.busy": "2024-04-30T16:47:45.382119Z",
     "iopub.status.idle": "2024-04-30T16:47:48.153998Z",
     "shell.execute_reply": "2024-04-30T16:47:48.153403Z",
     "shell.execute_reply.started": "2024-04-30T16:47:45.382348Z"
    },
    "language": "python"
   },
   "outputs": [],
   "source": [
    "vectorstore=SingleStoreDB.from_documents(documents=all_splits, table_name=\"test6\", embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T16:48:44.854397Z",
     "iopub.status.busy": "2024-04-30T16:48:44.854136Z",
     "iopub.status.idle": "2024-04-30T16:48:46.955494Z",
     "shell.execute_reply": "2024-04-30T16:48:46.954897Z",
     "shell.execute_reply.started": "2024-04-30T16:48:44.854376Z"
    },
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'How do you use Llama 3 models in the AWS console?',\n",
       " 'result': 'To use Llama 3 models in the AWS console, you can follow these steps:\\n\\n1. Log in to the AWS Management Console.\\n2. Navigate to the Amazon SageMaker console.\\n3. Click on the \"Models\" tab.\\n4. Click on the Llama 3 model you want to use.\\n5. In the \"Model details\" page, scroll down to the \"Use in a SageMaker notebook\" section.\\n6. Click on the \"Use\" button.\\n7. Review the following steps from the previous response.\\n\\nYou can also use the Llama 3 model to generate a new Llama 3 model to'}"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,retriever=vectorstore.as_retriever())\n",
    "qa_chain({\"query\": \"How do you use Llama 3 models in the AWS console?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "singlestore_cell_default_language": "python",
  "singlestore_connection": {
   "connectionID": "6efce35a-2db5-4ae3-bc0b-c3d5810a45f4",
   "defaultDatabase": "llama3_demo"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
